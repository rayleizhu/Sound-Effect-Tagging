{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(100)\n",
    "import csv\n",
    "from scipy import io\n",
    "import pickle\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAudio(info):\n",
    "    display(Audio(info['previews']['preview-lq-mp3']))\n",
    "\n",
    "def load_pickle(fname):\n",
    "    f = open(fname, 'rb')\n",
    "    out = pickle.load(f)\n",
    "    f.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags  = load_pickle('data/train_tags.pickle3')\n",
    "train_mfccs = load_pickle('data/train_mfccs.pickle3')\n",
    "train_info  = load_pickle('data/train_info.pickle3')\n",
    "\n",
    "test_mfccs = load_pickle('data/test_mfccs.pickle3')\n",
    "test_info  = load_pickle('data/test_info.pickle3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delta MFCCs\n",
    "def compute_delta_mfccs(mfccs):\n",
    "    dmfccs = []\n",
    "    for m in mfccs:\n",
    "        tmp = m[1:] - m[0:-1]\n",
    "        dm = hstack((m[0:-1], tmp))\n",
    "        dmfccs.append(dm)\n",
    "    return dmfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dmfccs = compute_delta_mfccs(train_mfccs)\n",
    "test_dmfccs  = compute_delta_mfccs(test_mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acoust: 100\n",
      "analog: 100\n",
      "bass: 160\n",
      "beat: 128\n",
      "drum: 371\n",
      "effect: 141\n",
      "electron: 194\n",
      "field: 110\n",
      "glitch: 110\n",
      "guitar: 130\n",
      "hit: 110\n",
      "loop: 237\n",
      "machin: 100\n",
      "metal: 117\n",
      "nois: 199\n",
      "percuss: 285\n",
      "record: 192\n",
      "space: 125\n",
      "synth: 220\n",
      "synthes: 136\n",
      "vocal: 120\n",
      "voic: 167\n"
     ]
    }
   ],
   "source": [
    "tagnames, tagnames_counts = unique(concatenate(train_tags), return_counts=True)\n",
    "for a,b in zip(tagnames, tagnames_counts):\n",
    "    print(\"{}: {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tags into binary class labels\n",
    "def tags2class(tags, tagnames):\n",
    "    b = zeros(shape=(len(tags), len(tagnames)))\n",
    "    for i,t in enumerate(tags):\n",
    "        for j,n in enumerate(tagnames):\n",
    "            if n in t:\n",
    "                b[i,j] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_classes[i,j] = absence/presence of the j-th tag in the i-th sound\n",
    "train_classes = tags2class(train_tags, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 160., 128., 371., 141., 194., 110., 110., 130., 110.,\n",
       "       237., 100., 117., 199., 285., 192., 125., 220., 136., 120., 167.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check we did this correctly...\n",
    "# it should be the same as the tag counts above\n",
    "sum(train_classes,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_csv_kaggle_tags(fname, tagnames, Yscores):\n",
    "    # header\n",
    "    tmp = [['Id']]\n",
    "    for t in tagnames:\n",
    "        tmp[0].append(t)    \n",
    "    \n",
    "    # add ID numbers for each Y, and usage if necessary\n",
    "    for i in range(len(Yscores)):\n",
    "        tmp2 = [(i+1)]\n",
    "        for t in range(len(tagnames)):\n",
    "            tmp2.append(Yscores[i,t])\n",
    "        \n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    f = open(fname, 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tmp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## YOUR CODE and DOCUMENTATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 22\n",
      "number of training samples: 1788\n",
      "number of testing samples: 262\n",
      "number of positives in each class:\n",
      " [100. 100. 160. 128. 371. 141. 194. 110. 110. 130. 110. 237. 100. 117.\n",
      " 199. 285. 192. 125. 220. 136. 120. 167.]\n",
      "train_mfccs[0].shape: (345, 13)\n",
      "train_classes.shape: (1788, 22)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(tagnames)\n",
    "num_train = len(train_mfccs)\n",
    "num_test = len(test_mfccs)\n",
    "pos_cnt = train_classes.sum(0)\n",
    "\n",
    "print(f'num_classes: {num_classes}')\n",
    "print(f'number of training samples: {num_train}')\n",
    "print(f'number of testing samples: {num_test}')\n",
    "print(f'number of positives in each class:\\n {pos_cnt}')\n",
    "\n",
    "\n",
    "print(f'train_mfccs[0].shape: {train_mfccs[0].shape}')\n",
    "print(f'train_classes.shape: {train_classes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# dataset\n",
    "from extra.dataset import MFCCDataset, MyPadCollate\n",
    "\n",
    "# train utils\n",
    "from extra.train_utils import train, evaluate, AUROC, \\\n",
    "                              grid_search, load_model\n",
    "\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "SEED = 10086\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/3\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_dataset = MFCCDataset(train_mfccs, train_classes)\n",
    "allloader = DataLoader(mfcc_dataset,\n",
    "                             batch_size=64,\n",
    "                             collate_fn=MyPadCollate(batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "train_val_split_ratio = 0.9\n",
    "train_num = int(train_val_split_ratio * len(mfcc_dataset))\n",
    "valid_num = len(mfcc_dataset) - train_num\n",
    "mfcc_train, mfcc_valid = random_split(mfcc_dataset,\n",
    "                                      (train_num, valid_num))\n",
    "\n",
    "print(len(mfcc_train))\n",
    "print(len(mfcc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(mfcc_train,\n",
    "                         batch_size=64,\n",
    "                         collate_fn=MyPadCollate(batch_first=True))\n",
    "\n",
    "validloader = DataLoader(mfcc_valid,\n",
    "                       batch_size=64,\n",
    "                       collate_fn=MyPadCollate(batch_first=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm scores are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc_auc_all: 0.9740, auc_all: 0.9741\n",
      "mc_auc_valid: 0.9658, auc_valid: 0.9673\n"
     ]
    }
   ],
   "source": [
    "best_save_path = 'ckpt_backup/gru_mcauc_0.864.pt'\n",
    "model = load_model(best_save_path)\n",
    "\n",
    "allloader = DataLoader(mfcc_dataset,\n",
    "                       batch_size=128,\n",
    "                       collate_fn=MyPadCollate(True)\n",
    "                      )\n",
    "\n",
    "pred_all, (mc_auc_all, auc_all) = evaluate(model, allloader,\n",
    "                                           AUROC(len(tagnames)))\n",
    "print(f'mc_auc_all: {mc_auc_all:.4f}, auc_all: {auc_all:.4f}')\n",
    "\n",
    "pred_valid, (mc_auc_valid, auc_valid) = evaluate(model, validloader,\n",
    "                                                AUROC(len(tagnames)))\n",
    "print(f'mc_auc_valid: {mc_auc_valid:.4f}, auc_valid: {auc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(tagnames)\n",
    "pseudo_test_label = np.random.randint(2, size=(len(test_mfccs), n))\n",
    "\n",
    "test_dataset = MFCCDataset(test_mfccs, pseudo_test_label)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=128,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=MyPadCollate(True))\n",
    "\n",
    "test_preds, _ = evaluate(model, test_loader,\n",
    "                         AUROC(len(tagnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = os.path.basename(best_save_path)[:-2]+'csv'\n",
    "write_csv_kaggle_tags(save_name, tagnames, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
